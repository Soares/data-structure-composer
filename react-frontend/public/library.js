window.ScalaBigOLibrary = {"implText":"\n// access by index\ngetFirst <- getByIndex\ngetNext <- getByIndex\ngetByIndex <- getFirst + n * getNext\ngetByIndex <- unorderedEach\ngetLast <- getByIndex\ngetPrev <- getByIndex\n\n// looping\nunorderedEach <- each\neach <- unorderedEach + nlogn\neach <- n * getByIndex\neach <- getFirst + n * getNext\n\n// insertions\ninsertLast! <- getLast + insertAfterEndNode!\ninsertLast! <- insertAtIndex!\ninsertFirst! <- insertAtIndex!\ninsertFirst! <- getFirst + insertBeforeFrontNode!\ninsertAtIndex! <- getByIndex + insertAfterNode!\ninsertAtIndex! <- getByIndex + insertBeforeNode!\ninsertAfterNode! <- insertNextToNode!\ninsertAfterEndNode! <- insertAfterNode!\ninsertBeforeNode! <- insertNextToNode!\ninsertBeforeFrontNode! <- insertBeforeNode!\ninsertNextToNode! <- insertAtIndex!\nextend! <- getLast + n * insertAfterEndNode!\ninsertAnywhere! <- insertAtIndex!\ninsertAnywhere! <- insertFirst!\ninsertAnywhere! <- insertLast!\n\n\n// deletions\ndeleteAtIndex! <- getByIndex + deleteNode!\ndeleteFirst! <- deleteAtIndex!\ndeleteFirst! <- getFirst + deleteNodeWhichIsFirst!\ndeleteLast! <- deleteAtIndex!\ndeleteLast! <- getLast + deleteNodeWhichIsLast!\ndeleteNodeWhichIsFirst! <- deleteNode!\ndeleteNodeWhichIsLast! <- deleteNode!\ndeleteBetweenNodes! <- n * deleteNode! + n * getNext\ndeleteAtIndex! <- deleteBetweenNodes!\n\ndeleteFirstNodeWithValue! <- getFirstNodeWithValue[valueOrdering] + deleteNode!\n\n// updates\nupdateFirstNode! <- getFirst + updateNodeWhichIsFirst!\nupdateNodeWhichIsFirst! <- updateNode!\nupdateFirstNode! <- deleteFirst! + insertBeforeFrontNode!\n\nupdateLastNode! <- getLast + updateNodeWhichIsLast!\nupdateNodeWhichIsLast! <- updateNode!\nupdateLastNode! <- deleteLast! + insertAfterEndNode!\n\nupdateByIndex! <- deleteAtIndex! + insertNextToNode!\nupdateByIndex! <- getByIndex + updateNode!\n\n// stuff about orderings by values\nquickSelect[f] <- unorderedEach\ngetKthBy[f] <- quickSelect[f]\ngetFirstBy[f] <- reduce[_{commutative, idempotent} <- f]\ngetFirstBy[f] <- getKthBy[f]\ngetLastBy[f] <- reduce[_{commutative, idempotent} <- f]\ngetLastBy[f] <- getKthBy[f]\ncountBetweenBy[f] <- unorderedEach\ngetMaximum <- getFirstBy[valueOrdering]\ngetMaximum <- getLastBy[valueOrdering]\ngetMinimum <- getLastBy[valueOrdering]\ngetMinimum <- getFirstBy[valueOrdering]\ndeleteFirstBy![f] <- getFirstBy[f] + deleteNode!\ndeleteLastBy![f] <- getLastBy[f] + deleteNode!\ndeleteMinimum! <- deleteFirstBy![valueOrdering]\ndeleteMaximum! <- deleteLastBy![valueOrdering]\ngetFirstNodeWithValue[f] <- unorderedEach\n// The obvious implementation for getNearest is to just loop over everything\ngetNearest[f] <- unorderedEach\n// using binary search:\ngetNearest[f] <- log(n) * getKthBy[f]\n\ngetRandomlyChosenElementWeightedByValue <- getSum + unorderedEach\n\n// other reductions\ncount <- unorderedEach\ncontains <- count\ncontains <- getFirstNodeWithValue[_]\nreduce[f] <- each + n * f\nreduce[f] if f.commutative <- unorderedEach + n * f\n\ncount <- countOfEquivalenceClass[_]\ncountOfEquivalenceClass[f] <- unorderedEach\nmostNumerousEquivalenceClass[f] <- unorderedEach\ngetMode <- mostNumerousEquivalenceClass[_]\n\n// eg, querying for the sum of the elements between indexes i and j\ntwoSidedIndexRangeQuery[reduction] <- unorderedEach\n\n// eg, querying for the sum of the elements between index 0 and index i\noneSidedIndexRangeQuery[reduction] <- unorderedEach\noneSidedIndexRangeQuery[reduction] <- twoSidedIndexRangeQuery[reduction]\nrangeMinimumQuery <- twoSidedIndexRangeQuery[_{idempotent}]\n\n// eg, querying for the number of elements whose values are in the range (a, b)\ntwoSidedValueRangeQuery[f, reduction] <- unorderedEach\n// eg, querying for the sum of the k smallest elements\noneSidedValueRangeQuery[f, reduction] <- unorderedEach\noneSidedValueRangeQuery[f, reduction] <- twoSidedValueRangeQuery[f, reduction]\n\nvalueOrdering <- 1\n\ngetSum <- reduce[_{commutative, invertible} <- 1]","dataStructureTexts":["ds StackReductionMemoizer[reduction] {\n    insertLast! <- 1\n    deleteLast! <- 1\n    reduce[reduction] <- 1\n    oneSidedIndexRangeQuery[reduction] <- 1\n}\n\n\nSuppose you want to maintain the minimum of a stack. You can do this by maintaining two\n stacks, as described eg [here](http://stackoverflow.com/a/685074/1360429).\n\nHere's an animated GIF:\n\n![animation of stack reduction memoizer](http://i.makeagif.com/media/11-13-2016/Xld7DV.gif)\n\n(Incidentally, you can build this data structure for a stack\n[with only O(1) space](http://www.geeksforgeeks.org/design-a-stack-that-supports-getmin-in-o1-time-and-o1-extra-space/).)","ds EquivalenceClassModeMemoizer[partitionFunction] extends HistogramHashMap {\n    updateNode! <- 1\n    deleteNode! <- 1\n    insertAtIndex! <- 1\n    countOfEquivalenceClass[partitionFunction] <- 1\n    mostNumerousEquivalenceClass[partitionFunction] <- 1\n}\n\nSuppose you want to be able to quickly answer queries like \"what's the most common element in this list\",\nor, more generally, \"what's the most common result of calling the partition function on elements in this list\".\n\nYou could do this in log time by storing a binary heap of all the different equivalence classes with\n a hash map from equivalence class to the item in the heap.\n\nBut in this case you can actually do it faster than that, by basically storing your equivalence classes in a\nsorted linked list of linked lists. For example, for the frequency distribution `{a: 5, b:4, c:4, d: 2, e:2, f: 1}`,\nyou can store the following kind of thing:\n\n\n    DoublyLinkedList(\n        (5, DoublyLinkedList(\"a\")),\n        (4, DoublyLinkedList(\"b\", \"c\")),\n        (2, DoublyLinkedList(\"d\", \"e\")),\n        (1, DoublyLinkedList(\"f\"))\n    )\n\nStore a pointer to the head and the tail of the list. And store a hash map from every equivalence class to\nits inner list. Now, you can add new equivalence classes in O(1) by just appending them to the\nlowest list. You can increment and decrement the count in O(1), because that just involves removing\na node from a doubly linked list and inserting it in another.\n\nI don't have an implementation for this at the moment; it seems kind of fiddly to write. Perhaps\nwriting this would be a fun interview question!","ds DequeReductionMemoizer[reduction] extends StackReductionMemoizer {\n    insertLast! <- 1\n    deleteLast! <- 1\n    insertFirst! <- 1\n    deleteFirst! <- 1\n    reduce[reduction] <- 1\n}\n\n\nLike a StackReductionMemoizer, but for a deque. You can make this the same way that you build a queue out of two stacks. So the DequeMemoizer\n would be built out of two stacks.\n\nIt could also be built out of two linked lists, which would have the same time\ncomplexity but requires more memory for pointers.","ds NoDeleteReductionMemoizer[reduction] {\n    insertLast! <- 1\n    insertFirst! <- 1\n    reduce[reduction] <- 1\n}\n\nIf you want to maintain the minimum of an array, and items are never deleted and are only ever inserted on the ends, it's easy: just store it in a variable and update it as things are added. This requires no nice properties of the function to work.","ds SparseTableForIdempotentReduction[reduction] if reduction.idempotent {\n    twoSidedIndexRangeQuery[reduction] <- 1 + reduction\n    insertLast! <- log(n)\n    deleteLast! <- 1\n}\n\nThis is a data structure for quickly answering queries of the form \"what's the minimum of the elements between indexes `i` and `j`\". It works for other reductions\nthan minimum, but they have to be idempotent.\n\nA complete explanation is available from slide 29 [here](http://web.stanford.edu/class/cs166/lectures/00/Small00.pdf).\n","ds TieredVector extends ArrayList {\n    getByIndex <- 1\n    updateNode! <- 1\n\n    insertAtIndex! <- sqrt(n)\n    insertLast! <- 1\n\n    // in the original paper, it doesn't make insertFirst! fast. But you can always just\n    // do the banker's queue trick\n    insertFirst! <- 1\n\n    deleteAtIndex! <- sqrt(n)\n\n    // you can delete at the ends easily\n    deleteLast! <- 1\n    deleteFirst! <- 1\n}\n\nA tiered vector is a fun data structure made from an array of arrays.\n[Here's the original paper about it.](http://www.ics.uci.edu/~goodrich/pubs/wads99.pdf)\nThe basic idea is that when you have n items,\nyou have a single layer of indirection to sqrt(n) arrays of size approximately sqrt(n). Inserting\nor deleting from those takes O(sqrt(n)) amortized.\n","ds ArrayList {\n    getByIndex <- 1\n    updateNode! <- 1\n\n    insertAtIndex! <- n\n    insertLast! <- 1\n\n    deleteAtIndex! <- n\n    deleteBetweenNodes! <- n\n\n    // you can delete at the end easily\n    deleteLast! <- 1\n}\n\nThis is a list backed by a [dynamic array](https://en.wikipedia.org/wiki/Dynamic_array).","ds BinaryHeap[heapOrdering] {\n    unorderedEach <- n\n    getFirstBy[heapOrdering] <- 1\n    deleteNode! <- log(n)\n    updateNode! <- log(n)\n    insertAtIndex! <- log(n)\n}\n\nBinary heaps are a well know data structure. They are not asymptotically faster than BSTs for\nmost of their operations, but they're significantly faster in practice, and they're much more\nspace-efficient.\n\nIt's an unordered data structure, so the only looping construct it supports is `unorderedEach`.","ds AliasArray {\n    getRandomlyChosenElementWeightedByValue <- 1\n    extend! <- n\n}\n\nThis uses the Alias method...\n\n","ds ValueOrderedAugmentedOrderStatisticTree[f, g] extends ValueOrderedOrderStatisticTree {\n    unorderedEach <- n\n    updateNode! <- log(n)\n    getFirstBy[f] <- 1\n    getLastBy[f] <- 1\n    getKthBy[f] <- log(n)\n    insertAtIndex! <- log(n)\n    getFirstNodeWithValue[f] <- log(n)\n    deleteNode! <- log(n)\n    countBetweenBy[f] <- log(n)\n    twoSidedValueRangeQuery[g] <- log(n)\n    oneSidedValueRangeQuery[g] <- log(n)\n    getNearest[f] <- log(n)\n    reduce[g] <- 1\n}\n\nThis is a balanced binary search tree ordered on `f`. Every node also maintains how many descendents it has,\nwhich makes it an order statistic tree. It might also be augmented with some other reduction `g` at each node,\nwhich speeds up queries about that reduction.\n\nMy favorite slides on order statistic trees and BST augmentation are [these ones](http://web.stanford.edu/class/cs166/lectures/06/Small06.pdf):\nread from slide 8.","ds InvertibleReductionMemoizer[f] if f.invertible {\n    reduce[f] <- 1\n    insertAtIndex! if f.commutative <- 1\n    insertFirst! <- 1\n    insertLast! <- 1\n    deleteFirst! <- 1\n    deleteLast! <- 1\n    updateNode! if f.commutative <- 1\n    deleteNode! if f.commutative <- 1\n}\n\nIf you want to maintain the sum of an array, it's super easy to do: just store it in a variable somewhere,\nand add new items to it as they're added to the array, and subtract items as they're removed.\n\nThis works because addition is invertible and commutative.\n\nThis data structure does that for any invertible function `f`.\n\nIf `f` is commutative, you can also insert, update, and delete anywhere in the list.","ds OrderStatisticTreeList {\n    getByIndex <- log(n)\n    updateNode! <- log(n)\n    getNext <- 1\n    getPrev <- 1\n    insertLast! <- 1\n    insertAfterNode! <- 1\n    insertFirst! <- 1\n    deleteNode! <- log(n)\n    deleteBetweenNodes! <- log(n)\n}\n\nOrder statistic trees are binary search trees where every node also stores its number of descendants. Because of that,\nit's fast to traverse them by index. Here's a picture\n(taken from [these notes](http://www.cs.cornell.edu/courses/cs211/2004su/slides/Topic20b.pdf)):\n\n![picture of OST](http://shlegeris.com/img/ost.png)\n\nThey are a really useful structure for ordered lists which need to support insertion at arbitrary locations.\n","ds QueueFromTwoStacks extends ArrayList {\n    getByIndex <- 1\n    updateNode! <- 1\n\n    insertAtIndex! <- n\n    insertLast! <- 1\n    insertFirst! <- 1\n\n    deleteAtIndex! <- n\n    deleteBetweenNodes! <- n\n\n    // you can delete at both ends easily\n    deleteLast! <- 1\n    deleteFirst! <- 1\n}\n\nA queue, [built from two stacks](http://stackoverflow.com/questions/69192/how-to-implement-a-queue-using-two-stacks).","ds HistogramHashMap[partitionFunction] {\n    countOfEquivalenceClass[partitionFunction] <- 1\n    insertAtIndex! <- 1\n    updateNode! <- 1\n    deleteNode! <- 1\n}\n\nA partition function is something which lets you group items into different equivalence classes. If\nyou want to be able to quickly count the number of elements in an equivalence class, you can have a\nhash map which stores how many elements are in each of your equivalence classes.","ds ValueOrderedOrderStatisticTree[f] {\n    unorderedEach <- n\n    updateNode! <- log(n)\n\n    // You can get the first and last elements in O(1) by storing a pointer to them\n    getFirstBy[f] <- 1\n    getLastBy[f] <- 1\n    getKthBy[f] <- log(n)\n    insertAtIndex! <- log(n)\n    getFirstNodeWithValue[f] <- log(n)\n    deleteNode! <- log(n)\n    countBetweenBy[f] <- log(n)\n    getNearest[f] <- log(n)\n}\n\nThis is a balanced binary search tree ordered on `f`. Every node also maintains how many descendents it has,\nwhich makes it an order statistic tree.\n\nMy favorite slides on order statistic trees and BST augmentation are [these ones](http://web.stanford.edu/class/cs166/lectures/06/Small06.pdf):\nread from slide 8.\n","ds AugmentedOrderStatisticTreeList[g] extends OrderStatisticTreeList {\n    getByIndex <- log(n)\n    updateNode! <- log(n)\n    getNext <- 1\n    getPrev <- 1\n    insertLast! <- log(n)\n    insertAfterNode! <- log(n)\n    insertFirst! <- log(n)\n    deleteNode! <- log(n)\n    deleteBetweenNodes! <- log(n)\n    twoSidedIndexRangeQuery[g] <- log(n)\n}\n"]}
